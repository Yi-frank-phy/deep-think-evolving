import asyncio
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv

load_dotenv()

# Use the new graph with evolution loop
from src.core.graph_builder import build_deep_think_graph
from src.logging_utils import emit_spec_event


def parse_args(argv: Optional[list[str]] = None) -> Namespace:
    parser = ArgumentParser(description="Run the Deep Think Evolving pipeline.")
    parser.add_argument(
        "--emit-spec-log",
        action="store_true",
        help="Persist spec-formatted logs to --spec-log-path after execution.",
    )
    parser.add_argument(
        "--spec-log-path",
        type=Path,
        default=Path("artifacts/spec_pipeline.log"),
        help="Path where spec logs should be written when --emit-spec-log is set.",
    )
    parser.add_argument(
        "--max-iterations",
        type=int,
        default=5,
        help="Maximum evolution iterations before termination.",
    )
    # NOTE: --temperature-coupling removed. LLM temperature is always 1.0.
    # System temperature Ï„ controls resource allocation only.
    return parser.parse_args(argv)


async def run_pipeline(args: Namespace) -> None:
    """Run the end-to-end Deep Think Evolving pipeline with evolution loop."""

    print("--- Running Deep Think Evolving Pipeline ---")

    problem_state = (
        "æˆ‘ä»¬æ­£åœ¨å¼€å‘ä¸€ä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹é©±åŠ¨çš„è‡ªä¸»ç ”ç©¶ä»£ç†ã€‚"
        "å½“å‰è¿›å±•ï¼šä»£ç†å¯ä»¥åˆ†è§£é—®é¢˜ã€æ‰§è¡Œç½‘ç»œæœç´¢å¹¶é˜…è¯»æ–‡æ¡£ã€‚"
        "é‡åˆ°çš„å›°å¢ƒï¼šå½“é¢å¯¹éœ€è¦ç»¼åˆæ¥è‡ªå¤šä¸ªæ¥æºçš„çŸ›ç›¾ä¿¡æ¯æ‰èƒ½å¾—å‡ºç»“è®ºçš„å¤æ‚é—®é¢˜æ—¶ï¼Œ"
        "ä»£ç†çš„æ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚å®ƒç»å¸¸ä¼šé™·å…¥å…¶ä¸­ä¸€ä¸ªä¿¡æºçš„è§‚ç‚¹ï¼Œæˆ–è€…æ— æ³•å½¢æˆä¸€ä¸ªè¿žè´¯çš„æœ€ç»ˆç­”æ¡ˆã€‚"
    )
    print(f"\nProblem State:\n{problem_state}")

    # Build and compile the graph with evolution loop
    app = build_deep_think_graph()

    # Initial state matching DeepThinkState
    initial_state = {
        "problem_state": problem_state,
        "strategies": [],
        "research_context": None,
        "spatial_entropy": float("inf"),  # Start high, converge low
        "effective_temperature": 1.0,
        "normalized_temperature": 0.5,
        "config": {
            "max_iterations": args.max_iterations,
            "entropy_threshold": 0.1,
            "total_child_budget": 6,
            "t_max": 2.0,
            "c_explore": 1.0,
            # NOTE: LLM temperature is always 1.0 (Logic Manifold Integrity)
        },
        "virtual_filesystem": {},
        "history": [],
        "iteration_count": 0,
    }

    print(f"\nConfig: max_iterations={args.max_iterations}")

    # Execute the graph (async for streaming support)
    try:
        final_state = await app.ainvoke(initial_state)
    except Exception as e:
        print(f"\nPipeline exited with error: {e}")
        import traceback
        traceback.print_exc()
        return

    # Process results
    print("\n--- Pipeline Results ---")
    print(f"Iterations completed: {final_state.get('iteration_count', 0)}")
    print(f"Final entropy: {final_state.get('spatial_entropy', 0):.4f}")
    print(f"Final temperature (tau): {final_state.get('normalized_temperature', 0):.4f}")
    
    # Output final report
    final_report = final_state.get("final_report")
    if final_report:
        print("\n" + "=" * 60)
        print("ðŸ“ æœ€ç»ˆæŠ¥å‘Š")
        print("=" * 60)
        print(final_report)
        print("=" * 60)

    
    strategies = final_state.get("strategies", [])
    active = [s for s in strategies if s.get("status") == "active"]
    expanded = [s for s in strategies if s.get("status") == "expanded"]
    
    print(f"Total strategies: {len(strategies)} ({len(active)} active, {len(expanded)} expanded)")
    
    # Show top strategies by score
    sorted_strategies = sorted(strategies, key=lambda x: x.get("score", 0), reverse=True)
    print("\nTop 5 Strategies:")
    for i, s in enumerate(sorted_strategies[:5]):
        print(f"  {i+1}. [{s.get('status', '?')}] {s['name']} (score: {s.get('score', 0):.3f})")

    # Process history logs
    history = final_state.get("history", [])
    
    def emit(message: str) -> None:
        print(message)

    if args.emit_spec_log:
        log_path = args.spec_log_path
        log_path.parent.mkdir(parents=True, exist_ok=True)
        spec_message = f"Spec log stored at {log_path.resolve()}"
        emit_spec_event(emit, "FILE", spec_message)
        log_path.write_text("\n".join(history) + "\n", encoding="utf-8")

    print("\n--- Pipeline Finished ---")


def main(argv: Optional[list[str]] = None) -> None:
    """Entry point for the Deep Think Evolving pipeline."""
    args = parse_args(argv)
    asyncio.run(run_pipeline(args))


if __name__ == "__main__":
    main()

