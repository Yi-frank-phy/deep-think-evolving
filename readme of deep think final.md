# **开发需求文档：一个“自适应进化压力”下的“混合智能体”生态系统**

版本: final draft v0 
日期: 2025年9月8日  
核心约束 (Core Constraint): 不可以涉及对于调用的LLM API内部参数的权重更新 (No modification to the internal weights of the called LLM APIs is allowed)。  
可控接口 (Controllable Interfaces): Google/GenAI SDK, LangChain, etc.
框架选择：建议全局使用langchain和langgraph，特别是其中的langchain google的包。但是，对于涉及到及其细粒度的控制，则直接改为Google/GenAI SDK，不建议使用其他框架，除非必须
Agent 自己可以使用的工具：mcp tools, groundings，

### **1\. 系统顶层设计哲学 (Guiding Principles)**
 
1.1. 从“系统设计”到“生态培育”  
1.2. 有引导的进化 (Guided Evolution)  
1.3. 经典与涌现的共生 (Symbiosis of Classic & Emergent)

### **2\. 高级系统架构：嵌套进化引擎 (Nested Evolution Engine)**

(本节内容整合自v3.2)  
为了在不改变模型权重的前提下提升智能上限，我们将系统设计为一个嵌套的双循环进化系统。  
**2.1. 内循环 (Inner Loop): 在线解法进化 (Online Solution Evolution)**

* **职责**: 针对一个具体问题，其实时推理过程本身就是一个高速的、在线的微型进化算法，旨在进化出该问题的最优解法。  
* **核心技术**: **并行进化搜索 (Parallel Evolutionary Search)**。类AlphaEvolve的遗传算法、模拟退火。

**2.2. 外循环 (Outer Loop): 离线策略进化 (Offline Strategy Evolution)**

* **职责**: 在后台运行，进化出更强大的**元策略 (Meta-Strategies)**，用于指导和配置内循环进化引擎。  
* **核心技术**: 类似agent fly 框架的对于记忆的训练

2.3. 嵌套统一关系 (Nested Unification)  
外循环进化出内循环的“进化规则”，内循环则在这些规则的指导下进化出具体问题的“解”。例如，外循环可以进化出用于Value\_Score评估的最佳Prompt模板，或者调整探索项的平衡系数C。  
2.4. 【继承自v3.1】循环接口 \- 启发式部署与验证协议 (Heuristic Deployment & Validation Protocol)  
外循环产生的新策略需经过策略暂存区 (Staging Area) 和 A/B测试，验证其有效性后才能部署到整个生态。

### **3\. 内循环：并行进化搜索引擎 (Parallel Evolutionary Search Engine)**

3.1. 核心Agent认知架构：模块化Agent设计  
(继承自v3.1，保持不变)

* **规划器 (Planner)**  
* **执行器 (Executor)**  
* **反思器 (Reflector)**  
* **记忆控制器 (Memory Controller)**

3.2. 先进的搜索算法: 并行进化搜索 (Parallel Evolutionary Search)  
本算法旨在解决LLM串行思维带来的多样性瓶颈，其核心是通过“人机协同”的模式，在关键节点进行可靠的策略分叉。  
**3.2.1. 核心挑战与对策 (Core Challenge & Strategy)**

* **挑战**: 完全依赖自主Agent（无论是单个还是多个）来生成高质量、多样化的战略分叉点，在工程上是脆弱且不可靠的。  
* **对策**: 我们采用一个\*\*“LLM生成可能性，人类进行决策 (LLM Proposes, Human Disposes)”\*\* 的人机协同流程，将LLM定位为“战略架构师”，而将人类专家定位为最终的“总设计师”。

3.2.2. 【v3.6 核心更新】新增：人机协同的策略分叉 (Human-in-the-Loop Strategy Forking)  
此机制由系统协调器 (System Orchestrator)（见5.1节）在关键节点（如初始步骤或搜索停滞时）启动和管理：

1. **架构师生成 (Architect Generation)**: 系统协调器调用一个专用的“**战略架构师 (Strategy Architect)**”LLM，其任务不是解决问题，而是生成一份包含所有可能方向的“战略蓝图”。

   【优化版提示词】  
   System Prompt:  
   你是一位'战略系统架构师' (Strategic Systems Architect)。你的主要职能是对复杂问题进行元层面分析。你不直接解决问题，而是识别并绘制出所有通往解决方案的基础战略路径。你的分析必须广博、多样，并专注于概念上截然不同的方法。  
   **User Prompt**: 分析以下问题状态：  
   \[问题状态：包含完整的上下文、当前进展，以及遇到的具体困境或决策点\]  
   你的任务是生成一份详尽的、包含所有从此状态出发的、相互排斥的战略方向清单。对每一个方向，请提供一个简洁的名称、清晰的理由和其所依赖的核心假设。  
   **约束**:

   1. ## **最大化多样性: 策略之间必须存在根本性差异。避免对同一核心思想的微小改动。**

   2. **仅限高层次**: 不要提供详细的程序步骤。专注于“做什么”和“为什么”，而不是“怎么做”。  
   3. **保持中立**: 不要对策略表示任何偏好或进行评估。你的角色是绘制蓝图，而非评判。

     
      **请将结果输出为单一的JSON对象数组。每个对象必须包含以下三个键**:

   * strategy\_name: 一个简短的、描述性的中文标签 (例如, "几何构造法")。  
   * rationale: 一句解释该策略核心逻辑的中文描述。  
   * initial\_assumption: 一句描述该策略若要可行所必须依赖的关键假设的中文描述。  
2. **人类确认与迭代 (Human Confirmation & Iteration)**: “战略架构师”生成的JSON输出，会通过**人类在环 (HIL) 接口**（见5.2节）呈现给人类专家。专家可以：  
   * **确认 (Confirm)**: 勾选所有他们认为可行或值得探索的战略方向。  
   * **修改 (Modify)**: 编辑某个策略的描述或其核心假设。  
   * **补充 (Add)**: 手动添加LLM未能想到的全新战略方向。  
   * 否决 (Reject): 要求系统基于新的指示重新生成蓝图。  
     这个“建议-返工-修改”的循环会一直持续，直到人类专家对战略分叉的最终方案感到满意。  
3. **强制并行实例化 (Forced Parallel Instantiation)**: 系统协调器接收到**经人类最终确认**的战略方向列表后，为每一个方向启动一个**全新的、独立的并行搜索线程**，并施加硬约束。

3.2.3. 【v3.5 核心修正】线程内的在线进化 (Intra-Thread Online Evolution)  
每一个并行线程内部，都独立运行基于种群熵的在线进化搜索算法。此算法是实现无偏UCB估计的关键。  
**3.2.3.1. 算法流程 (Algorithm Flow)**  
在每个推理步骤 (depth d):

1. **繁殖 (Propagation)**: 对当前种群（大小为k的波束）中的每个候选路径，调用“生成器”LLM，产生多个潜在的下一步，形成一个更大的“后代池 (offspring pool)”。  
2. 评估 (Evaluation): 对“后代池”中的每一个新候选者 c，计算其最终适应度分数：  
   Final\_Score(c) \= Value\_Score(c) \+ C \* Exploration\_Bonus(c)  
3. **选择 (Selection)**: 从“后代池”中，选择Final\_Score最高的k个候选者，形成深度为d+1的新一代种群（新波束）。

**3.2.3.2. 种群熵与探索奖励的量化 (Quantifying Population Entropy & Exploration Bonus)**

1. **概念指纹 (Concept Fingerprint)**: 通过Prompt工程，要求LLM在生成推理时，显式地输出其使用的关键“概念”或“方法”。  
2. **种群熵估计 (Population Entropy Estimation)**: 通过计算种群中所有“概念”的**香农熵 (Shannon Entropy)** 来度量多样性。  
3. 边际贡献探索奖励 (Marginal Contribution Exploration Bonus): 一个新候选者 c 的探索奖励，与其引入概念的稀有度成正比。  
   Exploration\_Bonus(c) \= Σ\_{i ∈ F(c)} (1 / (Count(i) \+ 1))

3.2.4. 【新增】高级探索机制：基于伊辛模型与模拟退火的智能剪枝

**3.2.4.1. 核心思想与类比**

本设计的核心思想是将“并行进化搜索”中的候选解种群视为一个物理系统，通过引入统计物理学的概念来对其宏观行为进行量化和引导。

- **核心类比**:
    - **“概念指纹” (Concepts)**  -> **“自旋” (Spins)**: 种群中的每一个基础解题思路或方法，都可被视为一个“自旋粒子”。
    - **“种群状态” (Population State)** -> **“系统微观态” (System Microstate)**: 整个种群所有候选解中“概念”的分布和组合，定义了系统的当前状态。
    - **“种群多样性” (Diversity)** vs **“收敛度” (Convergence)** -> **“系统能量” (System Energy)**: 系统的能量函数 E 被设计用来量化种群的“优劣”。一个理想的种群应该是在保持有益多样性的同时，逐步向高质量解收敛。
        - **高能量状态**: 种群混乱无序，概念多样性极高但解的质量普遍较低（类似高温下的无序磁体）。
        - **低能量状态**: 种群高度收敛，概念单一，解的质量很高（类似低温下自旋一致排列的磁体）。
    - **“进化压力” (Evolutionary Pressure)** -> **“温度 T” (Temperature)**: 温度是控制系统从探索到利用转变的关键参数。
        - **高温 (High T)**: 系统容忍高能量状态，允许“坏”的变异（即引入新概念但暂时降低了Value Score）存在，从而鼓励**探索**，避免陷入局部最优。
        - **低温 (Low T)**: 系统倾向于寻找并稳定在低能量状态，对任何可能增加系统能量的变异（如偏离当前最优解核心概念的尝试）进行抑制，从而鼓励**利用**。

- **最终目标**:
  通过**模拟退火（Simulated Annealing）**算法，我们控制“温度 T”从高到低缓慢下降。在这个“降温”过程中，系统的剪枝策略会从“宽容”变得“严苛”，自动地、平滑地将搜索重心从前期的广度探索，转移到后期的深度利用，最终稳定在高质量的解上。

**3.2.4.2. 设计蓝图 (Design Blueprint)**

**步骤 1: 定义系统的“能量函数” (Energy Function)**

我们需要一个函数 `E` 来评估当前种群（大小为k的波束）的整体状态。能量越低代表状态越优。一个可能的能量函数可以设计为：

`E_population = -w_v * Avg(Value_Score) + w_d * H(Concepts)`

- `Avg(Value_Score)`: 种群中所有候选解的 `Value_Score` 的平均值。我们希望这个值最大化，因此在能量函数中取其负值。
- `H(Concepts)`: 种群中所有“概念”的**香农熵 (Shannon Entropy)**。这代表了种群的多样性。熵越高，多样性越好。
- `w_v` 和 `w_d`: 分别是“价值权重”和“多样性权重”。这两个权重是超参数，用于平衡收敛速度和探索范围。

这个能量函数的核心在于：系统希望找到**Value Score高**（能量项 `-w_v * Avg(Value_Score)` 更负）且**多样性适中**（能量项 `w_d * H(Concepts)` 不会过大）的种群状态。

**步骤 2: 实现自适应温度控制器 (Adaptive Temperature Controller)**

根据您的最新指示，我们将放弃基于固定深度的温度衰减策略，转而采用一种更智能的、基于UCB（Upper Confidence Bound）思想的自adaptive温度控制机制。其核心是：**系统的“温度”应当直接反映其当前“不确定性”或“探索水平”**。

在我们的系统中，**种群的概念熵 `H(Concepts)`** 正是衡量系统多样性和不确定性的完美指标。

- **高温**: 当种群中概念多样性高（熵值大）时，意味着系统处于积极的探索阶段，此时温度`T`应该高，以鼓励对新路径的尝试。
- **低温**: 当种群收敛，概念趋于单一（熵值小）时，意味着系统进入了利用阶段，此时温度`T`应该低，以聚焦于对当前最优解的精炼。

**新的温度计算公式**:

`T = T_max * (H(Concepts) / H_max)`

-   `T`: 当前的实时系统温度。
-   `T_max`: 预设的最高温度，一个用于缩放的超参数。您的理解是正确的，它主要用于提供调控的灵活性，而非理论必需。
-   `H(Concepts)`: 当前种群的实时香non熵。
-   `H_max`: 理论上的最大可能熵，`log(N)`，其中 `N` 是系统中已知“概念”的总数。这用于将熵值归一化到 `[0, 1]` 区间。

这种设计使得“降温”过程不再是预设的、盲目的，而是由系统自身的进化状态驱动的有机过程，完美契合了您关于构建白盒化、高自由度系统的设计哲学。

**步骤 3: 制定基于Metropolis准则的剪枝规则**

根据您的决策，我们采纳方案A作为最终的剪枝实现路径。此方案将模拟退火的概率性接受则（Metropolis准则）直接应用于候选解的选择过程，是该思想最纯粹的体现。

**实现流程**:

1.  从“后代池”中选出 `Final_Score` 最高的 `k` 个作为基准优胜者（当前的最优种群）。
2.  对于池中其他分数较低的候选解 `c_low`，计算它与基准种群中最差的解 `c_worst` 的分数差：`ΔScore = Score(c_worst) - Score(c_low)`。（注意 `ΔScore` 是一个正值）。
3.  以 `P = exp(-ΔScore / T)` 的概率，用 `c_low` 替换 `c_worst`，将其纳入新一代的种群中。

**效果分析**:

-   **高温时 (T 很大)**: `-ΔScore / T` 趋近于0，接受概率 `P` 趋近于1。这意味着即使一个解比当前最优种群的“门槛”差很多，也极有可能会被保留下来。系统表现出极强的探索欲，愿意“赌”那些看似前景不佳但可能蕴含新思想的路径。
-   **低温时 (T 很小)**: `-ΔScore / T` 是一个很大的负数，接受概率 `P` 趋近于0。这意味着只有分数值非常接近优胜者的解才有一丝机会被“破格”保留。系统变得非常“挑剔”，倾向于在当前最优解周围进行精细的优化和利用。

**3.2.4.3. 优势与挑战**

**优势**

1.  **理论坚实**: 为探索与利用的平衡提供了一个经过充分验证的数学框架，使剪枝策略更加智能和自适应。
2.  **动态调整**: 系统能够根据“温度”（即进化阶段）自动调整其“风险偏好”，前期大胆探索，后期稳健收敛。
3.  **避免局部最优**: 在搜索初期通过保留看似“较差”但具有新颖“概念”的解，大大增加了跳出局部最优解陷阱的可能性。

**挑战**

1.  **超参数敏感**: 最高温度 `T_max`、能量函数中的权重 `w_v` 和 `w_d` 对算法性能至关重要，需要通过实验进行精细调整。
2.  **计算开销**: 实施该方案需要对后代池进行额外的概率计算。对整个种群计算香non熵也存在一定开销，需要优化。
3.  **能量函数定义**: 如何精确地将“概念”和 `Value_Score` 组合成一个能完美反映种群状态的“能量”，是理论与实践结合的难点。需要持续迭代和验证。

**3.2.4.4. 最终设计：与模型生成温度的动态耦合**

根据您的最终决策，我们确定将系统层面的“选择压力”（系统温度T）与模型层面的“生成创造性”（Gemini API的`temperature`参数）进行动态耦合。这是一个核心的设计，旨在使系统的两个关键环节（生成与选择）在探索/利用策略上保持高度同步。

-   **核心机制**: 在每次调用“生成器”LLM进行繁殖（Propagation）时，其`temperature`参数将不再是一个固定的常量，而是根据当前实时系统温度`T`动态设定。

-   **耦合公式**:
    `model_temperature = T_system`
    其中 `T_system = T_max * (H(Concepts) / H_max)`

-   **参数规范**:
    -   `model_temperature`: 传递给Gemini API的最终`temperature`值。
    -   `T_max`: 关键的人类可调超参数，用于设定探索强度的上限。
        -   **定义域**: `[0.0, 2.0]`，与Gemini API的`temperature`参数范围保持一致。
        -   **默认值**: `1.0`。如果不进行设置，系统将在`[0.0, 1.0]`的范围内进行温和的探索与利用。
        -   **效果**: 设置`T_max`为`2.0`将使系统在探索初期（熵最大时）以最高的创造性生成候选解；设置为`0.5`则会使整个过程更加稳健和收敛。

-   **协同效应**:
    -   **高熵（探索期）**: 系统温度`T`较高，模型`temperature`也随之升高，LLM会生成更多样、更大胆、更具创造性的“后代”，完美匹配系统在选择阶段的“宽容”策略。
    -   **低熵（利用期）**: 系统温度`T`降低，模型`temperature`也随之降低，LLM生成的内容会更加确定、聚焦和优化，与系统在选择阶段的“严苛”策略形成合力，共同对最优解进行精炼。

这一设计确保了从“思想的诞生”到“思想的筛选”，整个进化链条都遵循着同一个由种群熵驱动的自适应节律。

3.2.5. 全局协调 (Global Coordination)  
系统协调器监控所有并行线程，并可进行资源重新分配或共享关键发现。

### **4\. 外循环：先进记忆系统：从经验中学习**
* **4.1. 混合记忆架构**: 短期、情景、语义、程序化记忆。  
* **4.2. 目标导向型记忆检索**: 参数化记忆、情节控制。

### **5\. 系统交互与监控接口：Prometheus 控制塔**

本系统的核心交互与监控由一个统一的“控制塔”UI负责，旨在将人类专家从被动的“操作员”提升为主动的“总设计师”。其设计哲学、核心组件（并行进化沙盘、KPI仪表盘、情境式干预面板）与关键工作流的详细定义，请参见独立的《UI/UX设计稿.md》文档。

### **6\. 总结**

本项目旨在构建一个能够自我进化的智能生态。**v3.6的最终升级是用一个“人机协同的策略分叉”流程，取代了所有先前版本中关于如何生成多样性的复杂自主机制。** 通过将LLM定位为“战略架构师”进行广度生成，并将最终的决策权和创造力补充交由人类专家，我们构建了一个更简单、更鲁棒、也更强大的系统。这个框架完美地结合了机器的计算能力和人类的深层智慧，为解决真正开放和困难的问题提供了最坚实的基础。

另外，搜索引擎默认优先使用gemini的Google search Grounding，这个grounding部分不要使用全局的langchain，langgraph框架，直接使用genai SDK，方便search结果的metadata的传输，添加metadata处理的逻辑，通过标准的引用将可能需要的搜索链接引用回传回来