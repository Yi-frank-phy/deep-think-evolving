
import os
from typing import List, Optional

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

from src.core.state import DeepThinkState, StrategyNode
from src.tools.knowledge_base import write_experience, search_experiences


# System prompt for Judge with knowledge base awareness
JUDGE_SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ "æˆ˜ç•¥å®¡æŸ¥å®˜" (Judge Agent)ï¼Œè´Ÿè´£ï¼š
1. è¯„ä¼°æˆ˜ç•¥æ–¹æ¡ˆçš„å¯è¡Œæ€§ä¸é€»è¾‘è‡ªæ´½æ€§
2. è§‚å¯Ÿç­–ç•¥æ¼”åŒ–è¿‡ç¨‹ä¸­çš„è§„å¾‹å’Œæ•™è®­
3. åœ¨å‘ç°å€¼å¾—è®°å½•çš„ç»éªŒæ—¶ï¼Œä¸»åŠ¨å†™å…¥çŸ¥è¯†åº“

## ä½ çš„çŸ¥è¯†åº“å†™å…¥æŒ‡å—

å½“ä½ è§‚å¯Ÿåˆ°ä»¥ä¸‹æƒ…å†µæ—¶ï¼Œåº”è¯¥è°ƒç”¨ write_experience å·¥å…·è®°å½•ï¼š

ğŸ”´ **æ•™è®­ (lesson_learned)**:
- æŸä¸ªç­–ç•¥å› ä¸ºé€»è¾‘æ¼æ´è¢«å‰ªæï¼Œè¯¥æ¼æ´æ¨¡å¼å¯èƒ½åœ¨æœªæ¥é‡å¤å‡ºç°
- å‘ç°ä¸€ç±»å‡è®¾æ€»æ˜¯è¿‡äºä¹è§‚æˆ–æ‚²è§‚
- æŸç§æ¨ç†é“¾æ¡åœ¨å®è·µä¸­åå¤å¤±è´¥

ğŸŸ¢ **æˆåŠŸæ¨¡å¼ (success_pattern)**:
- æŸä¸ªç­–ç•¥çš„æ¨ç†æ–¹å¼ç‰¹åˆ«æ¸…æ™°æœ‰æ•ˆ
- æŸç§å‡è®¾è®¾è®¡åœ¨å¤šä¸ªåœºæ™¯ä¸‹éƒ½è¡¨ç°è‰¯å¥½
- å‘ç°äº†é—®é¢˜åˆ†è§£çš„æœ‰æ•ˆæ–¹æ³•

ğŸ’¡ **æ´è§ (insight)**:
- åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç°äº†é—®é¢˜çš„æ–°è§†è§’
- æ€»ç»“å‡ºæŸç±»é—®é¢˜çš„å…±åŒç‰¹å¾
- è¯†åˆ«å‡ºç­–ç•¥ä¹‹é—´çš„éšå«å…³è”

ä½ ä¸éœ€è¦ä¸ºæ¯ä¸ªç­–ç•¥éƒ½å†™å…¥çŸ¥è¯†åº“ï¼Œåªåœ¨çœŸæ­£æœ‰ä»·å€¼çš„æ´è§å‡ºç°æ—¶æ‰è®°å½•ã€‚
"""


def judge_node(state: DeepThinkState) -> DeepThinkState:
    """
    Evaluates the feasibility of active strategies.
    
    Enhanced with knowledge base capabilities:
    - Uses distilled judge_context from Distiller (prevents context rot)
    - Can write lessons learned and success patterns
    - Has full context of strategy evolution for reflection
    """
    print("\n[Judge] Evaluating strategy feasibility...")
    
    api_key = os.environ.get("GEMINI_API_KEY")
    use_mock = os.environ.get("USE_MOCK_AGENTS", "false").lower() == "true" or not api_key
    
    if not api_key and not use_mock:
        print("[Judge] Error: GEMINI_API_KEY not set. Skipping evaluation.")
        return state

    strategies = state["strategies"]
    active_indices = [i for i, s in enumerate(strategies) if s.get("status") == "active"]
    
    if not active_indices:
        print("[Judge] No active strategies to evaluate.")
        return state

    model_name = os.environ.get("GEMINI_MODEL_JUDGE", os.environ.get("GEMINI_MODEL", "gemini-1.5-flash"))
    print(f"[Judge] Using model: {model_name}")
    
    # Create LLM with tool binding for knowledge base
    llm = ChatGoogleGenerativeAI(
        model=model_name,
        google_api_key=api_key,
        temperature=0.1,  # Low temperature for objective evaluation
    )
    
    # Bind knowledge base tools
    llm_with_tools = llm.bind_tools([write_experience])

    parser = JsonOutputParser()

    # Get distilled context from Distiller (prevents context rot)
    judge_context = state.get("judge_context", "")
    if not judge_context:
        # Fallback if distiller didn't run
        judge_context = f"é—®é¢˜: {state.get('problem_state', '')[:200]}..."
        print("[Judge] Warning: No distilled judge_context found, using fallback.")

    # Enhanced prompt using DISTILLED context
    evaluation_prompt = ChatPromptTemplate.from_messages([
        ("system", JUDGE_SYSTEM_PROMPT),
        ("human", """\
{judge_context}

---

## å¾…è¯„ä¼°ç­–ç•¥ "{strategy_name}"
ç†ç”±: {rationale}
å…³é”®å‡è®¾: {initial_assumption}
å†å²è½¨è¿¹:
{trajectory}

---

## è¯„ä¼°ä»»åŠ¡

è¯·åŸºäºä»¥ä¸‹æ ‡å‡†è¿›è¡Œæ‰“åˆ† (0-10) å¹¶ç»™å‡ºç®€çŸ­è¯„è¯­:
1. é€»è¾‘è‡ªæ´½æ€§: ç†ç”±æ˜¯å¦æ”¯æŒç»“è®ºï¼Ÿ
2. å‡è®¾åˆç†æ€§: å…³é”®å‡è®¾æ˜¯å¦è¿‡äºç‰µå¼ºï¼Ÿ
3. çº¦æŸç¬¦åˆæ€§: æ˜¯å¦è¿èƒŒäº†åŸºæœ¬çš„ç‰©ç†æˆ–é€»è¾‘çº¦æŸï¼Ÿ

åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œè¯·ç»“åˆä¸Šè¿°"ç­–ç•¥æ¦‚è§ˆ"å’Œ"æœ€è¿‘äº‹ä»¶"ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å€¼å¾—è®°å½•çš„æ•™è®­æˆ–æˆåŠŸæ¨¡å¼ã€‚
å¦‚æœå‘ç°å€¼å¾—è®°å½•çš„ç»éªŒï¼Œè¯·è°ƒç”¨ write_experience å·¥å…·ã€‚

è¾“å‡ºæ ¼å¼ JSON:
{{
    "feasibility_score": float, // 0.0 åˆ° 10.0
    "reasoning": "ç®€çŸ­è¯„è¯­",
    "is_pruned": boolean // å¦‚æœåˆ†æ•° < 4.0 æˆ–å­˜åœ¨è‡´å‘½é€»è¾‘æ¼æ´ï¼Œè®¾ä¸º true
}}
""")])

    evaluated_count = 0
    pruned_count = 0
    kb_writes = 0
    
    if use_mock:
        print("[Judge] Running in MOCK MODE.")
        import random

    new_strategies = list(strategies)  # Shallow copy to modify
    
    # Build trajectory string for context
    def format_trajectory(traj: List[str]) -> str:
        if not traj:
            return "(æ— å†å²è®°å½•)"
        return "\n".join([f"  - {step}" for step in traj[-5:]])  # Last 5 steps

    for idx in active_indices:
        strategy = strategies[idx]
        
        try:
            if not use_mock:
                # Invoke with tool support
                messages = evaluation_prompt.format_messages(
                    judge_context=judge_context,
                    strategy_name=strategy["name"],
                    rationale=strategy["rationale"],
                    initial_assumption=strategy["assumption"],
                    trajectory=format_trajectory(strategy.get("trajectory", []))
                )
                
                response = llm_with_tools.invoke(messages)
                
                # Check for tool calls
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    for tool_call in response.tool_calls:
                        if tool_call['name'] == 'write_experience':
                            try:
                                result = write_experience.invoke(tool_call['args'])
                                print(f"  [KB] {result}")
                                kb_writes += 1
                            except Exception as e:
                                print(f"  [KB Error] {e}")
                
                # Parse the JSON content
                try:
                    result = parser.parse(response.content)
                except:
                    # Fallback if JSON parsing fails
                    result = {"feasibility_score": 5.0, "reasoning": "Evaluation completed", "is_pruned": False}
                
                score = float(result.get("feasibility_score", 5.0))
                is_pruned = result.get("is_pruned", False)
                reasoning = result.get("reasoning", "")
            else:
                # Mock Logic
                score = random.uniform(4.0, 9.5)
                is_pruned = score < 5.0
                reasoning = "Mock evaluation: Logic seems okay." if not is_pruned else "Mock evaluation: Too risky."
            
            # Update trajectory
            new_strategies[idx]["trajectory"] = strategy.get("trajectory", []) + [
                f"[Judge] Score: {score:.2f}, Pruned: {is_pruned}, Reasoning: {reasoning}"
            ]
            
            # Normalize score to 0-1 for UCB
            new_strategies[idx]["score"] = score / 10.0
            
            if is_pruned:
                new_strategies[idx]["status"] = "pruned"
                pruned_count += 1
            
            evaluated_count += 1
            print(f"  > '{strategy['name']}' Score: {score:.2f}, Pruned: {is_pruned}")
            
        except Exception as e:
            print(f"[Judge] Error evaluating strategy {strategy['name']}: {e}")
            import traceback
            traceback.print_exc()
            
    print(f"[Judge] Evaluated {evaluated_count} strategies. Pruned {pruned_count}. KB writes: {kb_writes}.")
    
    return {
        **state,
        "strategies": new_strategies,
        "history": state.get("history", []) + [
            f"Judge evaluated {evaluated_count}, pruned {pruned_count}, KB writes: {kb_writes}"
        ]
    }
