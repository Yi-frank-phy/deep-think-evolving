"""
Knowledge Base Tools for Outer Loop Memory System.

è®¾è®¡åŸåˆ™ (2024-12-14 é‡æ„):
1. Agent è‡ªä¸»å†³å®šå†™å…¥ - åªæœ‰ LLM çœŸæ­£è®¤ä¸ºæœ‰ä»·å€¼æ—¶æ‰è°ƒç”¨ write å·¥å…·
2. åªæœ‰ä¸¤ç§ä¿å­˜åœºæ™¯:
   - ç¡¬å‰ªææ—¶ä¿å­˜æœ‰ä»·å€¼çš„åˆ†æ”¯ä¿¡æ¯ (write_strategy_archive)
   - å…¨å±€ä¿å­˜ LLM è®¤ä¸ºå€¼å¾—å­¦ä¹ çš„æŠ½è±¡ç»éªŒ (write_experience)
3. å¬å›é˜ˆå€¼åŸºäºå‘é‡ç©ºé—´è·ç¦» Îµ (bandwidth):
   - è·ç¦» > 1Îµ: ä¸åº”å¬å› (è¶…å‡ºä¸€ä¸ªæ ‡å‡†å·®)
   - åªå¬å› distance < Îµ çš„é«˜åº¦ç›¸å…³ç»éªŒ

The knowledge base is a vectorized file system that persists across sessions.
"""

import os
import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any

import numpy as np
from langchain_core.tools import tool

from src.embedding_client import embed_text
from src.math_engine.kde import estimate_bandwidth


# Default knowledge base directory
DEFAULT_KB_PATH = Path("knowledge_base")


def get_kb_path() -> Path:
    """Get the knowledge base directory path from env or default."""
    kb_path = Path(os.environ.get("KNOWLEDGE_BASE_PATH", DEFAULT_KB_PATH))
    kb_path.mkdir(parents=True, exist_ok=True)
    return kb_path


def calculate_vector_distance(vec_a: List[float], vec_b: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚"""
    a = np.array(vec_a, dtype=float)
    b = np.array(vec_b, dtype=float)
    return float(np.linalg.norm(a - b))


def get_current_epsilon(embeddings: List[List[float]]) -> float:
    """
    åŸºäºå½“å‰åµŒå…¥é›†åˆä¼°è®¡ Îµ (å¸¦å®½)ã€‚
    
    Îµ ä»£è¡¨å‘é‡ç©ºé—´ä¸­çš„"ä¸€ä¸ªæ ‡å‡†å·®"è·ç¦»ã€‚
    å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„åµŒå…¥æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼ã€‚
    """
    if len(embeddings) < 2:
        return 1.0  # é»˜è®¤å€¼
    
    embeddings_array = np.array(embeddings, dtype=float)
    return estimate_bandwidth(embeddings_array)


@tool
def write_experience(
    title: str,
    content: str,
    experience_type: str,
    tags: Optional[List[str]] = None,
    related_strategy: Optional[str] = None,
) -> str:
    """
    å°†çœŸæ­£æœ‰ä»·å€¼çš„ç»éªŒå†™å…¥çŸ¥è¯†åº“ã€‚
    
    âš ï¸ é‡è¦: åªæœ‰å½“ä½ ç¡®ä¿¡è¿™æ˜¯ä¸€ä¸ªå€¼å¾—é•¿æœŸä¿å­˜çš„æ™®éæ€§ç»éªŒæ—¶æ‰è°ƒç”¨æ­¤å·¥å…·ã€‚
    ä¸è¦ä¸ºæ¯ä¸ªç­–ç•¥è¯„ä¼°éƒ½è°ƒç”¨æ­¤å·¥å…·ã€‚
    
    é€‚åˆä¿å­˜çš„ç»éªŒç±»å‹:
    - å¯æ³›åŒ–çš„æŠ½è±¡æ•™è®­ (ä¸æ˜¯å…·ä½“é—®é¢˜çš„å…·ä½“ç­”æ¡ˆ)
    - åˆ†æ”¯å†³ç­–çš„å…ƒç­–ç•¥ (å¦‚ä½•å†³å®šä½•æ—¶æ¢ç´¢ vs åˆ©ç”¨)
    - åå¤å‡ºç°çš„å¤±è´¥æ¨¡å¼ (å¯åœ¨æœªæ¥é—®é¢˜ä¸­é¿å…)
    
    Args:
        title: ç®€çŸ­çš„æè¿°æ€§æ ‡é¢˜
        content: æŠ½è±¡åŒ–çš„ç»éªŒæè¿° (é¿å…åŒ…å«å…·ä½“é—®é¢˜ç»†èŠ‚)
        experience_type: "lesson_learned", "success_pattern", "branching_heuristic", "meta_insight"
        tags: å¯é€‰çš„æ ‡ç­¾åˆ—è¡¨
        related_strategy: å¯é€‰çš„ç›¸å…³ç­–ç•¥åç§°
        
    Returns:
        ç¡®è®¤æ¶ˆæ¯
    """
    kb_path = get_kb_path()
    
    # éªŒè¯ experience_type
    valid_types = {"lesson_learned", "success_pattern", "branching_heuristic", "meta_insight"}
    if experience_type not in valid_types:
        return f"Error: experience_type must be one of {valid_types}"
    
    # Generate unique filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    short_id = str(uuid.uuid4())[:8]
    safe_title = "".join(c if c.isalnum() or c in "_ -" else "_" for c in title)[:50]
    filename = f"{timestamp}_{experience_type}_{safe_title}_{short_id}.json"
    
    # Build experience record (è½»é‡åŒ– - ä¸å­˜å‚¨å®Œæ•´åµŒå…¥)
    experience = {
        "id": str(uuid.uuid4()),
        "title": title,
        "content": content,
        "type": experience_type,
        "tags": tags or [],
        "related_strategy": related_strategy,
        "created_at": datetime.now().isoformat(),
        "metadata": {
            "source": "agent_autonomous_decision",
            "version": "2.0"
        }
    }

    # Generate embedding (ç”¨äºè¯­ä¹‰æœç´¢)
    embedding_text = f"{title}\n{content}"
    embedding = embed_text(embedding_text)
    if embedding:
        experience["embedding"] = embedding
    
    # Write to file
    file_path = kb_path / filename
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(experience, f, ensure_ascii=False, indent=2)
    
    print(f"[KB] Experience saved: {file_path.name}")
    return f"Experience saved: {file_path.name}"


def write_strategy_archive(
    strategy: Dict[str, Any],
    synthesis_context: str,
    branch_rationale: str,
    report_version: int
) -> str:
    """
    åœ¨ç¡¬å‰ªææ—¶å½’æ¡£æœ‰ä»·å€¼çš„ç­–ç•¥ä¿¡æ¯ã€‚
    
    è¿™æ˜¯ç¡¬å‰ªææµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ˜¯ç”± Agent è‡ªä¸»è°ƒç”¨çš„å·¥å…·ã€‚
    åªä¿å­˜åˆ†æ”¯å†³ç­–é€»è¾‘å’ŒæŠ½è±¡ç»éªŒï¼Œä¸ä¿å­˜å®Œæ•´çš„ç­–ç•¥å†…å®¹ã€‚
    
    Args:
        strategy: è¢«å‰ªæçš„ç­–ç•¥èŠ‚ç‚¹
        synthesis_context: ç»¼åˆä¸Šä¸‹æ–‡ (ä¸ºä»€ä¹ˆè¿™ä¸ªç­–ç•¥è¢«ç»¼åˆ)
        branch_rationale: åˆ†æ”¯å†³ç­–ç†ç”± (ä¸ºä»€ä¹ˆé€‰æ‹©äº†è¿™ä¸ªæ–¹å‘)
        report_version: æŠ¥å‘Šç‰ˆæœ¬å·
        
    Returns:
        ç¡®è®¤æ¶ˆæ¯
    """
    kb_path = get_kb_path()
    
    # Generate unique filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    short_id = str(uuid.uuid4())[:8]
    safe_name = "".join(c if c.isalnum() or c in "_ -" else "_" for c in strategy.get("name", "unknown"))[:30]
    filename = f"{timestamp}_branch_archive_{safe_name}_{short_id}.json"
    
    # åªä¿å­˜åˆ†æ”¯å†³ç­–çš„æŠ½è±¡ç»éªŒï¼Œè€Œéå…·ä½“ç­–ç•¥å†…å®¹
    archive = {
        "id": str(uuid.uuid4()),
        "type": "branch_archive",
        "title": f"åˆ†æ”¯å†³ç­–: {strategy.get('name', 'Unknown')}",
        "content": json.dumps({
            "strategy_name": strategy.get("name"),
            "branch_rationale": branch_rationale,  # å…³é”®: ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ–¹å‘
            "final_score": strategy.get("score", 0),
            "synthesis_context": synthesis_context[:500],  # æˆªæ–­ä»¥ä¿æŒè½»é‡
            "report_version": report_version
        }, ensure_ascii=False),
        "tags": ["branch_decision", f"report_v{report_version}"],
        "created_at": datetime.now().isoformat(),
        "metadata": {
            "source": "hard_pruning",
            "version": "2.0",
            "original_strategy_id": strategy.get("id")
        }
    }
    
    # åªä¸ºåˆ†æ”¯å†³ç­–ç†ç”±ç”ŸæˆåµŒå…¥ (æ›´è½»é‡)
    embedding_text = f"åˆ†æ”¯å†³ç­–: {branch_rationale}"
    embedding = embed_text(embedding_text)
    if embedding:
        archive["embedding"] = embedding
    
    # Write to file
    file_path = kb_path / filename
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(archive, f, ensure_ascii=False, indent=2)
    
    print(f"[KB] Branch archived: {strategy.get('name')} -> {file_path.name}")
    return f"Branch archived: {file_path.name}"


def _search_experiences_impl(
    query: str,
    query_embedding: Optional[List[float]] = None,
    current_embeddings: Optional[List[List[float]]] = None,
    experience_type: Optional[str] = None,
    limit: int = 3,
    epsilon_threshold: float = 1.0,  # è·ç¦»é˜ˆå€¼: 1Îµ = ä¸€ä¸ªæ ‡å‡†å·®
) -> List[Dict[str, Any]]:
    """
    åŸºäºå‘é‡è·ç¦»æœç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ç»éªŒ (å†…éƒ¨å®ç°)ã€‚
    
    âš ï¸ åªå¬å›è·ç¦» < epsilon_threshold * Îµ çš„é«˜åº¦ç›¸å…³ç»éªŒã€‚
    è¿™ç¡®ä¿äº†ä¸Šä¸‹æ–‡çš„çº¯å‡€æ€§ï¼Œé¿å…å¬å›ä¸ç›¸å…³çš„è…çƒ‚ä¸Šä¸‹æ–‡ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢æ–‡æœ¬
        query_embedding: å¯é€‰çš„é¢„è®¡ç®—æŸ¥è¯¢åµŒå…¥
        current_embeddings: å½“å‰ç­–ç•¥ç©ºé—´çš„åµŒå…¥ (ç”¨äºè®¡ç®— Îµ)
        experience_type: å¯é€‰çš„ç±»å‹è¿‡æ»¤
        limit: æœ€å¤§è¿”å›æ•°é‡
        epsilon_threshold: è·ç¦»é˜ˆå€¼å€æ•° (1.0 = 1Îµ, 0.25 = 1/4Îµ)
        
    Returns:
        åŒ¹é…çš„ç»éªŒåˆ—è¡¨ (åªè¿”å›é«˜åº¦ç›¸å…³çš„)
    """
    kb_path = get_kb_path()
    
    if not kb_path.exists():
        return []
    
    # è®¡ç®—æŸ¥è¯¢åµŒå…¥
    if query_embedding is None:
        query_embedding = embed_text(query)
    
    if not query_embedding:
        print("[KB] Warning: Could not generate query embedding")
        return []
    
    # è®¡ç®—å½“å‰ç©ºé—´çš„ Îµ (å¦‚æœæœ‰åµŒå…¥æ•°æ®)
    if current_embeddings and len(current_embeddings) >= 2:
        epsilon = get_current_epsilon(current_embeddings)
    else:
        # ä½¿ç”¨é»˜è®¤ Îµ (åŸºäºé«˜ç»´ç©ºé—´çš„å…¸å‹è·ç¦»)
        epsilon = 10.0  # é«˜ç»´ç©ºé—´çš„ä¿å®ˆé»˜è®¤å€¼
    
    distance_threshold = epsilon_threshold * epsilon
    print(f"[KB] Searching with Îµ={epsilon:.4f}, threshold={distance_threshold:.4f}")
    
    experiences = []
    
    # Load all experience files
    for file_path in kb_path.glob("*.json"):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                exp = json.load(f)
                
                # Filter by type if specified
                if experience_type and exp.get("type") != experience_type:
                    continue
                
                exp_embedding = exp.get("embedding")
                if not exp_embedding:
                    continue  # è·³è¿‡æ²¡æœ‰åµŒå…¥çš„ç»éªŒ
                
                # è®¡ç®—å‘é‡è·ç¦»
                distance = calculate_vector_distance(query_embedding, exp_embedding)
                
                # åªä¿ç•™è·ç¦» < é˜ˆå€¼çš„ç»éªŒ
                if distance < distance_threshold:
                    experiences.append({
                        "title": exp.get("title"),
                        "type": exp.get("type"),
                        "content": exp.get("content")[:300] if exp.get("content") else "",
                        "tags": exp.get("tags"),
                        "distance": distance,
                        "score": 1.0 - (distance / distance_threshold),  # å½’ä¸€åŒ–ç›¸å…³æ€§ (å…¼å®¹æµ‹è¯•)
                        "relevance": 1.0 - (distance / distance_threshold)  # å½’ä¸€åŒ–ç›¸å…³æ€§
                    })
                    
        except Exception as e:
            print(f"[KB] Warning: Error loading {file_path.name}: {e}")
            continue
    
    # æŒ‰è·ç¦»æ’åº (æœ€è¿‘çš„ä¼˜å…ˆ)
    experiences.sort(key=lambda x: x.get("distance", float("inf")))
    experiences = experiences[:limit]
    
    if experiences:
        print(f"[KB] Found {len(experiences)} relevant experiences (closest distance: {experiences[0]['distance']:.4f})")
    else:
        print(f"[KB] No experiences within distance threshold ({distance_threshold:.4f})")
    
    return experiences


@tool
def search_experiences(
    query: str,
    experience_type: Optional[str] = None,
    limit: int = 3,
) -> str:
    """
    åŸºäºå‘é‡è·ç¦»æœç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ç»éªŒã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢æ–‡æœ¬
        experience_type: å¯é€‰çš„ç±»å‹è¿‡æ»¤ ("lesson_learned", "success_pattern", ç­‰)
        limit: æœ€å¤§è¿”å›æ•°é‡
        
    Returns:
        JSON æ ¼å¼çš„ç»éªŒåˆ—è¡¨ï¼Œæˆ– "No matching experiences found."
    """
    results = _search_experiences_impl(
        query=query,
        experience_type=experience_type,
        limit=limit
    )
    
    if not results:
        return "No matching experiences found."
    
    return json.dumps(results, ensure_ascii=False, indent=2)


def format_experiences_for_context(experiences: List[Dict[str, Any]]) -> str:
    """
    Format experiences list into a string for LLM context injection.
    
    åªç”¨äºé«˜åº¦ç›¸å…³çš„ç»éªŒã€‚
    """
    if not experiences:
        return ""
    
    lines = ["## ç›¸å…³å†å²ç»éªŒ (é«˜åº¦ç›¸å…³)\n"]
    
    for i, exp in enumerate(experiences, 1):
        exp_type = exp.get("type", "unknown")
        type_label = {
            "lesson_learned": "ğŸ”´ æ•™è®­",
            "success_pattern": "ğŸŸ¢ æˆåŠŸæ¨¡å¼",
            "branching_heuristic": "ğŸ”€ åˆ†æ”¯å¯å‘",
            "meta_insight": "ğŸ’¡ å…ƒæ´è§",
            "branch_archive": "ğŸ“¦ åˆ†æ”¯å½’æ¡£"
        }.get(exp_type, exp_type)
        
        relevance = exp.get("relevance", 0)
        lines.append(f"### {i}. [{type_label}] {exp.get('title', 'Untitled')} (ç›¸å…³åº¦: {relevance:.1%})")
        lines.append(exp.get("content", "")[:300])
        lines.append("")
    
    return "\n".join(lines)
